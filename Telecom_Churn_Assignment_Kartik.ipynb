{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telecom Churn Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "For many incumbent operators, ___retaining high profitable customers is the number one business goal___.\n",
    "\n",
    "To reduce customer churn, __telecom companies need to predict which customers are at high risk of churn__.\n",
    "\n",
    "In this project, we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Importing Required Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings as w\n",
    "w.filterwarnings('ignore')\n",
    "pd.set_option('max_columns',500)\n",
    "pd.set_option('max_rows',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('telecom_churn_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As checked above, there are 214 numeric columns and 12 non-numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at data statistics\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In churn prediction, we assume that there are three phases of customer lifecycle :\n",
    "\n",
    "The ‘good’ phase [Month 6 & 7]<br>\n",
    "The ‘action’ phase [Month 8]<br>\n",
    "The ‘churn’ phase [Month 9]<br><br>\n",
    "In this case, since we are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for Checking missing values percentages\n",
    "def checkMissingPercent(dataset, cutoff):\n",
    "    missing = round(100*(dataset.isnull().sum()/dataset.shape[0]))\n",
    "    return missing.loc[missing>cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for imputing data \n",
    "def imputeData(df, col_list):\n",
    "    for i in [x + y for y in ['_6','_7','_8','_9'] for x in col_list]:\n",
    "        df[i].fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Handling missing values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_data=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since mobile no has all unique values and represents a particular customer, it can be dropped from the dataset.\n",
    "# Similarly, circle_id has all same values(109), it also can be dropped.\n",
    "mod_data.drop(['mobile_number', 'circle_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at missing value ratio in each column\n",
    "checkMissingPercent(mod_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As checked above, there are so many columns conatining missing values. Among them, there are some columns which has more than 70% of missing values. We will not directly delete those columns. Let us first check that these values as null because of no transactions or because of some other reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all columns for month of June which has 75% missing values\n",
    "cols = checkMissingPercent(mod_data, 74).index\n",
    "\n",
    "mod_data.loc[mod_data.date_of_last_rech_data_6.isna(),cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As checked above, all the columns has null values where date of last recharge is missing. This is valid, we can replace these null values with 0 as there is no recharge done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing all the columns other than those containg date with 0 having more than 50% missing value\n",
    "cols = list(filter(lambda x : not x.startswith('date') , checkMissingPercent(mod_data, 50).index))\n",
    "\n",
    "mod_data[cols]=mod_data[cols].apply(lambda x: x.fillna(0))\n",
    "mod_data[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking again percent of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkMissingPercent(mod_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the non-numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=mod_data.select_dtypes(include='object')\n",
    "for i in obj.columns:\n",
    "    print(i,'', obj[i].nunique(),'', obj[i].isna().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already used date to fill the missing values. Further these date columns seems to be irrelevant in our analysis, so we will drop these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod_data = mod_data.drop(obj.columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again checking for the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkMissingPercent(mod_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=list(checkMissingPercent(mod_data, 0).index)\n",
    "mod_data[cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As checked above, all the columns have their minimum value 0, but since the missing percent is very low around 4-5%, this can be because of technical or human error, its better to fill these values with median rather than 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filling the columns above with median\n",
    "mod_data[cols]=mod_data[cols].apply(lambda x: x.fillna(x.median()))\n",
    "mod_data[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if our missing value imputation is successfully done or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(mod_data.isna().sum()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates from row\n",
    "mod_data.drop_duplicates(inplace=True)\n",
    "mod_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few columns whose names are not consistent with other columns. Let make them same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list(filter(lambda x: x[-1].isalpha(), mod_data.columns)))\n",
    "mod_data.rename(columns={'aug_vbc_3g':'vbc_3g_8', 'jul_vbc_3g':'vbc_3g_7', 'jun_vbc_3g':'vbc_3g_6'}, inplace=True)\n",
    "mod_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Taking only the data of high valued customer by taking average of total recharge amount of good months__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_data['av_rech_amt_6_7']=((mod_data.av_rech_amt_data_6 * mod_data.total_rech_data_6 + mod_data.total_rech_amt_6)+\n",
    "                             (mod_data.av_rech_amt_data_7 * mod_data.total_rech_data_7 + mod_data.total_rech_amt_7)) / 2\n",
    "\n",
    "# mod_data.drop(['av_rech_amt_data_6','total_rech_data_6','total_rech_amt_6','av_rech_amt_data_7',\n",
    "#                'total_rech_data_7','total_rech_amt_7'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "high_value_cust = mod_data[mod_data.av_rech_amt_6_7>mod_data.av_rech_amt_6_7.quantile(0.7)]\n",
    "len(high_value_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_cust.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tagging the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:**\n",
    "<br>\n",
    "    1. total_ic_mou_9\n",
    "    2. total_og_mou_9\n",
    "    3. vol_2g_mb_9\n",
    "    4. vol_3g_mb_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_cust['churn'] = (high_value_cust.total_ic_mou_9+high_value_cust.total_og_mou_9 + high_value_cust.vol_3g_mb_9 + high_value_cust.vol_2g_mb_9).apply(lambda x: 1 if x==0 else 0)\n",
    "high_value_cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_cust.churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('churn rate:', round((2433/27520)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has high class imbalance, we will take care of it while building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "high_value_cust.drop(list(filter(lambda x: x[-1]=='9',high_value_cust.columns)), axis=1, inplace=True)\n",
    "high_value_cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_value_cust.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As checked in the data dictionary, columns start with fb and night are schemes which are used for facebook and night packs respectively, so they are categorical columns(yes/no). Same as with churn columns. We will convert then to object type. This will help in doing EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=list(filter(lambda x: x.startswith('fb') or x.startswith('night'), high_value_cust.columns))\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.append('churn')\n",
    "high_value_cust[cols]=high_value_cust[cols].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to add or subtract 2 columns to form a new column. It also have a feature to add or subtract 2 columns \n",
    "# based on complete column name or a pattern provided.\n",
    "    # col_a_end_str - column name or pattern from end for column A\n",
    "    # col_b_end_str - column name or pattern from end for column B\n",
    "    # avg_or_diff - 'avg' for average and 'diff' for subtraction of 2 columns\n",
    "    # new_name_end_str - end pattern to give to new column/s\n",
    "    # dataframe - a dataframe\n",
    "    # complete_column_name_given - flag to check if willing to merge all the columns based on pattern or not    \n",
    "\n",
    "def addOrSubColumns(col_a_end_str, col_b_end_str, avg_or_diff, new_name_end_str, dataframe, complete_column_name_given=False):\n",
    "    li=[]\n",
    "    if complete_column_name_given:\n",
    "        new_name= col_a_end_str+'_'+col_b_end_str+'_'+new_name_end_str\n",
    "        if avg_or_diff=='diff':\n",
    "                dataframe[new_name]= (dataframe[col_b_end_str] - dataframe[col_a_end_str])\n",
    "        else:\n",
    "            dataframe[new_name]= (dataframe[col_b_end_str] + dataframe[col_a_end_str])/2\n",
    "        print(new_name)\n",
    "        li+=[col_a_end_str,col_b_end_str]\n",
    "\n",
    "    else:\n",
    "        s=set(filter( lambda x: x[-len(col_a_end_str):]==col_a_end_str, dataframe.columns))\n",
    "        s1=set(filter( lambda x:  x[-len(col_b_end_str):]==col_b_end_str, dataframe.columns))\n",
    "        \n",
    "        for i in list(s):\n",
    "            k=i[:-len(col_a_end_str)]\n",
    "            a=k+col_a_end_str\n",
    "            b=k+col_b_end_str\n",
    "            if  b in s1:\n",
    "                if avg_or_diff=='diff':\n",
    "                    dataframe[k+new_name_end_str]= (dataframe[b] - dataframe[a])\n",
    "                else:\n",
    "                    dataframe[k+new_name_end_str]= (dataframe[b] + dataframe[a])/2\n",
    "                li+=[a,b]\n",
    "                s.remove(a); s1.remove(b)\n",
    "        \n",
    "    return dataframe.drop(li, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Univariate Analysis ---- #\n",
    "def univariate(dataset,col):\n",
    "    #col = dataset.columns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if dataset[col].dtypes != 'object':\n",
    "        sns.distplot(dataset[col])\n",
    "        dataset[col].describe()\n",
    "    else:\n",
    "        sns.countplot(dataset[col])\n",
    "        dataset[col].value_counts()\n",
    "    plt.title( 'Frequency Plot of ' + str(col) , loc='left', fontsize=12, fontweight=0, color='Blue')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Bivariate Analysis ---- #\n",
    "def bivariate(dataset, col1, col2):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if (dataset[col1].dtypes == 'object' and dataset[col2].dtypes != 'object'):\n",
    "        sns.boxplot(x = col1, y = col2, data = dataset)\n",
    "        plt.xlabel(col1)\n",
    "        plt.ylabel(col2)\n",
    "    elif (dataset[col1].dtypes != 'object' and dataset[col2].dtypes == 'object'):\n",
    "        sns.boxplot(x = col2, y = col1, data = dataset)\n",
    "        plt.xlabel(col2)\n",
    "        plt.ylabel(col1)\n",
    "    plt.title( 'Box Plot of ' + str(col1)+ ' vs '+ str(col2) , loc='left', fontsize=12, fontweight=0, color='Blue')\n",
    "    plt.show()\n",
    "    \n",
    "#         if dataset[col2].nunique()>10:\n",
    "#                 g.set_xticklabels(g.get_xticklabels(),rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Bivariate Analysis with Churn as one of the column ---- #\n",
    "def bivariate_churn(dataset,col):\n",
    "    if dataset[col].dtypes != 'object':\n",
    "        sns.boxplot(col, 'churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to cap outliers\n",
    "def capingOutliers(dataframe, quantile, columns, cap=False):\n",
    "    for i in columns:\n",
    "        print('outliers in',i, ':', len(dataframe[i][dataframe[i]>dataframe[i].quantile(quantile)]))\n",
    "        if cap:\n",
    "            dataframe[i][dataframe[i]>dataframe[i].quantile(quantile)] = dataframe[i].quantile(quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vs_Churn(dataset,col):\n",
    "    # per month churn vs Non-Churn\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "     \n",
    "    colList=list(data.filter(regex=(col)).columns)\n",
    "    colList = colList[:3]\n",
    "    plt.plot(high_value_cust.groupby('churn')[colList].mean().T)\n",
    "    ax.set_xticklabels(['Jun','Jul','Aug'])\n",
    "    \n",
    "    ## Add legend\n",
    "    plt.legend(['Non-Churn', 'Churn'])\n",
    "    \n",
    "    # Add titles\n",
    "    plt.title( str(col) +\" V/S Month\", loc='left', fontsize=12, fontweight=0, color='orange')\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(col)\n",
    "    plt.show()\n",
    "    \n",
    "    # Numeric stats for per month churn vs Non-Churn\n",
    "    return high_value_cust.groupby('churn')[colList].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_vs_Churn(high_value_cust,'total_ic_mou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__\n",
    "1. Total incoming calls drops at a faster pace for the churners from the month of June to July.\n",
    "2. For non-churners the graph is almost constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_vs_Churn(high_value_cust,'total_og_mou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__\n",
    "1. Total outgoing calls drops significantly for the churners from the month of June to July. We could also see that churners were quite higher in number than non churners in making outgoing calls in the month of June.\n",
    "2. For non-churners the graph is remains constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_vs_Churn(high_value_cust,'fb_user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__\n",
    "1. As observed, the number of fb users dropped for the churners from the month of June to July. \n",
    "2. For non-churners the graph is significantly constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vs_Churn(high_value_cust,'total_rech_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__\n",
    "1. Total recharge amount drops significantly for the churners from the month of June to July. We have also observed that churners were quite spending higher amount in recharging than non churners in the month of June.\n",
    "2. For non-churners the graph is almost constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_vs_Churn(high_value_cust,'max_rech_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__\n",
    "1. Maximum recharge amount drops for the churners from the month of June to July and it dropped at a steep rate to August.\n",
    "2. For non-churners the graph is almost constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_vs_Churn(high_value_cust,'arpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__\n",
    "1. Average Revenue Per User drops at a faster pace for the churners from the month of June to July.The ARPU from the churners was quite higher than the non-churners in the month of June.\n",
    "2. While for non-churners the graph is almost constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_vs_Churn(high_value_cust,'night_pck_user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__\n",
    "1. Night pack users drops significantly for the churners from the month of June to July.\n",
    "2. For non-churners the graph is fairly constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After analysis we do not need these columns as we have got a derived column av_rech_amt_6_7\n",
    "mod_data.drop(['av_rech_amt_data_6','total_rech_data_6','total_rech_amt_6','av_rech_amt_data_7',\n",
    "               'total_rech_data_7','total_rech_amt_7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(high_value_cust,'aon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(high_value_cust,'av_rech_amt_6_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate(high_value_cust,'av_rech_amt_6_7','churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier Treatment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "round(high_value_cust.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots to analyse the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# call the function to check and cap the outliers\n",
    "cols=list(high_value_cust.select_dtypes(exclude='object').columns) # columns to remove ouliers\n",
    "capingOutliers(high_value_cust, 0.95, cols, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating the columns of good months\n",
    "high_value_cust=addOrSubColumns('6','7','avg','6_7',high_value_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting average recharge amount for action month\n",
    "high_value_cust['av_rech_amt_8']=(high_value_cust.av_rech_amt_data_8 * high_value_cust.total_rech_data_8 + \n",
    "                                  high_value_cust.total_rech_amt_8)\n",
    "\n",
    "high_value_cust.drop(['av_rech_amt_data_8','total_rech_data_8','total_rech_amt_8'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# difference of the columns between action month and average of good month\n",
    "high_value_cust=addOrSubColumns('6_7','8','diff','diff',high_value_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- do some analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the columns having more than 85% of values as a single value (highly skewed columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "li=[]\n",
    "for i in high_value_cust.columns:\n",
    "    if max(high_value_cust[i].value_counts())/len(high_value_cust) >0.85:\n",
    "        li.append(i)\n",
    "\n",
    "li.remove('churn')\n",
    "high_value_cust.drop(li, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "high_value_cust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import sensitivity_specificity_support\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_cust.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into train and test\n",
    "X = high_value_cust.drop(\"churn\", axis = 1)\n",
    "y = high_value_cust.churn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes of train and test sets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pca to train data\n",
    "pca = Pipeline([('scaler', MinMaxScaler()), ('pca', PCA())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)\n",
    "churn_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract pca model from pipeline\n",
    "pca = pca.named_steps['pca']\n",
    "\n",
    "# look at explainded variance of PCA components\n",
    "print(pd.Series(np.round(pca.explained_variance_ratio_.cumsum(), 4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature variance\n",
    "features = range(pca.n_components_)\n",
    "cumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)\n",
    "plt.figure(figsize=(175/20,100/20)) # 100 elements on y-axis; 175 elements on x-axis; 20 is normalising factor\n",
    "plt.plot(cumulative_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "PCA_VARS = 20\n",
    "steps = [('scaler', MinMaxScaler()),\n",
    "         (\"pca\", PCA(n_components=PCA_VARS)),\n",
    "         (\"logistic\", LogisticRegression(class_weight='balanced'))\n",
    "        ]\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# check score on train data\n",
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# # check sensitivity and specificity\n",
    "# sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "# print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA()\n",
    "\n",
    "# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "logistic = LogisticRegression(class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [(\"scaler\", MinMaxScaler()), \n",
    "         (\"pca\", pca),\n",
    "         (\"logistic\", logistic)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [20, 30], 'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# # check sensitivity and specificity\n",
    "# sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "# print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "forest = RandomForestClassifier(class_weight={0:0.1, 1: 0.9}, n_jobs = -1)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {\"criterion\": ['gini', 'entropy'], \"max_features\": ['auto', 0.4]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# # check sensitivity and specificity\n",
    "# sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "# print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a random forest model on train data\n",
    "max_features = int(round(np.sqrt(X_train.shape[1])))    # number of variables to consider to split each node\n",
    "print(max_features)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_features=max_features, class_weight={0:0.1, 1: 0.9}, oob_score=True, random_state=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB score\n",
    "rf_model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# # check sensitivity and specificity\n",
    "# sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "# print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predictors\n",
    "features = high_value_cust.drop('churn', axis=1).columns\n",
    "\n",
    "# feature_importance\n",
    "importance = rf_model.feature_importances_\n",
    "\n",
    "# create dataframe\n",
    "feature_importance = pd.DataFrame({'variables': features, 'importance_percentage': importance*100})\n",
    "feature_importance = feature_importance[['variables', 'importance_percentage']]\n",
    "\n",
    "# sort features\n",
    "feature_importance = feature_importance.sort_values('importance_percentage', ascending=False).reset_index(drop=True)\n",
    "print(\"Sum of importance=\", feature_importance.importance_percentage.sum())\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 'n' features\n",
    "top_n = 10\n",
    "top_features = feature_importance.variables[0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature correlation\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] =(15,10)\n",
    "# mycmap = sns.diverging_palette(199, 359, s=99, center=\"light\", as_cmap=True)\n",
    "sns.heatmap(data=X_train[top_features].corr(), center=0.0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "steps = [('scaler', MinMaxScaler()), \n",
    "         (\"logistic\", LogisticRegression(class_weight={0:0.1, 1:0.9}))\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# # check sensitivity and specificity\n",
    "# sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "# print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
